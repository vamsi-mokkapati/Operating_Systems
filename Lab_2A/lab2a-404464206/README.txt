Vamsi Mokkapati
Student ID: 404-464-206
CS 111, Project 2A
TA: Zhaoxing Bu

Descriptions of Included Files:

lab2_add.c:
This is the program that is used to take thread number and iteration number as inputs,
in addition to yield and sync options, and use each thread to add 1 or -1 to a counter
while keeping track of the total counter value. The --yield option will cause a thread
to immediately yield rather than waiting for the end of a time slice to preempt it.
The sync option has 3 possible inputs for types of protection: a pthread_mutex (m),
a test-and-set spin lock (s), or a compare-and-swap (c). 

SortedList.h:
This is the provided header file that declares the functions SortedList_insert,
SortedList_delete, SortedList_lookup, and SortedList_length so we can perform
actions on a circular doubly linked list.

SortedList.c:
This is the file that implements the four doubly linked list functions declared in
SortedList.h (insert, delete, lookup, length). By using each node's prev and next
pointers and manipulating them, we allow elements to be added, removed, or looked
up. Traversing through the whole circular list also allows us to find the length.

lab2_list.c:
This is the program that is used to take thread number and iteration number as inputs,
in addition to yield and sync options, and use each thread to insert and delete
nodes (elements) into a doubly linked list while keeping track of list length. The
yield options i, d, and l are used to force the program to yield at the insert, delete,
and lookup functions respectively, and the sync function is used to enable either a
pthread_mutex (m) protection or a test-and-set spin lock (s) protection. 

Makefile:
Builds all the deliverables, output, tarball, and the graphs. Contains all the tests
required to generate the CSV files, can make the final tarball, and clean all programs
and output that is generated. Also is able to use the gnuplot executable to create all
the required graphs.

lab2_add.csv:
Comma separated value file generated by the 'tests' function of the Makefile, which
puts the lab2_add.c code under various tests for correctness. This file is referenced
in lab2_add.gp, which is used as an input for the gnuplot executable, which creates
all the lab2_add graphs.

lab2_list.csv:
Comma separated value file generated by the 'tests' function of the Makefile, which
puts the lab2_list.c code under various tests for correctness. This file is referenced
in lab2_list.gp, which is used as an input for the gnuplot executable, which creates
all the lab2_list graphs.

lab2_add-1.png:
Shows the threads and iterations required to generate a failure, with and without
yields. It's clearly seen that as the number of threads increases, it takes fewer
iterations to generate a failure.

lab2_add-2.png:
Shows the average time per operation with and without yields, with 2 and 8 threads. It
is seen that the cost per operation is higher with yields for both the thread numbers.

lab2_add-3.png:
Shows the average time per single threaded operation (without yields)  as a function of
iteration number.

lab2_add-4.png:
Shows the threads and iterations that can run successfully with yields under the 3
synchronization methods (compare-and-swap, mutex, and spin).

lab2_add-5.png:
Shows the average time per multithreaded operation as a function of the thread number for
all four versions of the add function - unprotected, compare-and-swap, mutex, and spin.

lab2_list-1.png:
For a single thread, this shows the average time per unprotected operation as a function of
iteration number. This shows the correction of the per-operation cost for the list length.

lab2_list-2.png:
This shows the threads and iterations required to generate a failure with and without yields.
It does this with the yield options i, d, il, and dl.

lab2_list-3.png:
Shows the protected iterations that run without failure compared with unprotected, for
a thread count of 12.

lab2_list-4.png:
Provides the length-adjusted average time per operation as a function of thread number for
mutex, spin-lock, and unprotected test cases.

README.txt:
This file gives descriptions of each of the included files, and contains all the answers
to the given questions in the spec (included below).

_____________________________________________________________________________________________________
Analysis:

QUESTION 2.1.1 - causing conflicts:
Why does it take many iterations before errors are seen?
Why does a significantly smaller number of iterations so seldom fail?

After running my program with a range of numbers of threads and iterations,
I noticed that for a single thread, no number of iterations would cause a failure.
For 2 threads, I would consistently get failure for 1000 iterations. For 4 threads,
I would also get failure for 1000 iterations. For 8 and 12 threads, I would get
failure at 100 iterations itself.

A larger number of iterations indicates that there will be more interruptions at
critical sections, causing race conditions; this is why the counter value will be
non-zero, and therefore incorrect. It takes many iterations before errors are seen
because for small iteration numbers, there won't be as many collisions within critical
sections, which results in fewer race conditions and correct output.

Having a much smaller iteration value will seldom fail because of the directly
proportional relationship between number of collisions and number of iterations when
there is no mutex or locking mechanism put in place. Smaller iteration numbers indicates
lower collisions happening, and will therefore be fewer race conditions and fewer
instances of failure.

QUESTION 2.1.2 - cost of yielding:
Why are the --yield runs so much slower?  Where is the additional time going?  Is
it possible to get valid per-operation timings if we are using the --yield option?
If so, explain how.  If not, explain why not.

The --yield runs are much slower because we are causing a context switch to be performed
at every invocation of the "add" function when we use the yield option. Context switches
are very expensive because we are asking the scheduler to save all state and register data
into a TLB and run the remaining part of the code on a different thread. This is why the
yield option takes so much additional time.

It is NOT possible to get valid per-operation timings with the --yield option due to the
large overhead caused due to context switches. The time it takes to switch between threads
is added to the per-operation time value, which is not accurate.

QUESTION 2.1.3 - measurement errors:
Why does the average cost per operation drop with increasing iterations?
If the cost per iteration is a function of the number of iterations, how do we know
how many iterations to run (or what the “correct” cost is)?

The average cost per operation drops with increasing iterations because of the large cost
of creating new threads compared with the cost of conducting each operation. A larger number
of iterations means less time is used in creating threads, and therefore the average cost
per operation decreases.

We can calculate how many iterations to run by taking the total elapsed time and dividing it
by the number of iterations to find the cost per iteration. For each amount of threads, we can
simply run the program on multiple iteration values, find which run has the lowest cost per
iteration, and look at the total elapsed time value to find our "correct" cost. If we subtract
the overhead time taken for new thread creation, then we realize that this cost is just the time
taken to execute the "add" function.

QUESTION 2.1.4 - costs of serialization:
Why do all of the options perform similarly for low numbers of threads?
Why do the three protected operations slow down as the number of threads rises?
Why are spin-locks so expensive for large numbers of threads?

With low numbers of threads, there is less parallelism involved in how the functions are executed;
therefore, the locking mechanisms are used less, and there are much fewer instances of collision.
The more or less sequential fashion of running results in similar performances for low amounts of
threads used.

When more threads are used with the three protected operations, the locking mechanisms take more
time to ensure mutual exclusion. Since only one thread can run at a time within the critical
sections, a larger amount of threads have to be kept waiting. This, combined with the increased
overhead of the locking mechanisms, results in the slowing down of operation with more threads.

The process of spinning takes up many time slices and wastes the CPU time, and since an increase of
threads means more threads have to keep spinning while one only one thread is running at the
critical section, the amount of overhead is increased drastically, which results in spin-locks
being very expensive.


QUESTION 2.2.1 - scalability of Mutex
Compare the variation in time per protected operation vs the number of threads
(for mutex-protected operations) in Part-1 and Part-2, commenting on
similarities/differences and offering explanations for them.

For part 1, when mutex-protected operations are used in conjunction with the
yield option, the times per protected operation vary from 400 to 1000 ns for
a range of 2 to 12 threads. When the same operations are used without the yield
option, the times per protected operation are between 35 and 200 ns for a range
of 1 to 12 threads, with the maximum average time at 4 threads.

For part 2, the average times drastically increase compared to what we found
in part 1, with the times per protected operation going from 1200 to 35000 ns
for a range of 1 to 24 threads, with the maximum average time occurring at the
highest thread count.

This larger average time per protected operation for part 2 can be attributed
to the fact that the locks are held for a much longer time in the linked-list
code, due to more complicated and longer critical sections. Therefore, more
threads are blocked at a given time so that context switches are not performed
during the time a thread is in a critical section. The threads are competing
for resources a lot more in part 2 than for the simple addition operations found
in part 1. One similarity between the variations in average time in part 1 and
2 is that as the number of threads increase, the average time is increasing, albeit
on a different scale. This can be attributed to increased overhead due to the
locking mechanisms when there are more threads.

QUESTION 2.2.2 - scalability of spin locks
Compare the variation in time per protected operation vs the number of threads
for Mutex vs Spin locks, commenting on similarities/differences and offering
explanations for them.

For mutex-protected operations in lab 2, we find that average time goes from
1200 to 35000 ns for a range of 1 to 24 threads, while for spin locks, we find
the average time goes from 1200 to almost 91000 ns for the same range of threads.
In both cases, we find that the longest time per protected operation occurs at
the highest thread count of 24 threads.

After comparison, it is obvious that using test-and-set spin locks to ensure
mutual exclusion is far less efficient than using pthread_mutexes. The reason for
this is that spin locks waste CPU time and take up many time slices due to the
fact that many threads have to keep spinning while one thread is running at
the critical section, which increases overhead much more compared to
pthread_mutexes. Since pthread_mutexes sleep when the thread is currently not
running instead of constantly spinning, they avoid the busy waiting conundrum
and are therefore more efficient in most cases.

Again, a similarity between mutex and spin locks is that as the number of threads
increase, the average time per operation increases, due to increased locking
overhead as thread count increases.
