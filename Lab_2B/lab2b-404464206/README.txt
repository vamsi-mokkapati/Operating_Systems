Vamsi Mokkapati
Student ID: 404-464-206
CS 111, Project 2B
TA: Zhaoxing Bu

Descriptions of Included Files:

lab2_add.c:
This file from project 2A is included to help generate lab2b_1.png, which
compares synchronized add and list operations. lab2_add.c is compiled and
run in my Makefile under the 'tests' section. 

SortedList.h:
This is the provided header file that declares the functions SortedList_insert,
SortedList_delete, SortedList_lookup, and SortedList_length so we can perform
actions on a circular doubly linked list.

SortedList.c:
This is the file that implements the four doubly linked list functions declared in
SortedList.h (insert, delete, lookup, length). By using each node's prev and next
pointers and manipulating them, we allow elements to be added, removed, or looked
up. Traversing through the whole circular list also allows us to find the length.

lab2_list.c:
In addition to the thread number, iteration number, yield, and sync options
enabled in 2A, we now add a --lists=# option, which allows a specified number of
sub-lists to be created and chosen based on a hash of the key. 

Makefile:
Builds all the deliverables, output, tarball, and the graphs. Contains all the tests
required to generate the lab_2b_list.csv file, and can also make the execution
profiling report profile.gperf. It has the 'clean' function to clean all executables
and generated output and the 'graph' function to use the gnuplot executable to create
all the required graphs.

lab_2b_list.csv:
Comma separated value file generated by the 'tests' function of the Makefile, which
puts the lab2_list.c code under various tests for correctness, including new tests for
the --lists=# option. These tests are designed specifically so that this csv file has
the necessary information for lab_2b_list.gp can use for graphing.

lab_2b_list.gp:
This file is what is used as input to the gnuplot executable to generate the 5 required
graphs. I used regex expressions to look through the lab_2b_list.csv file and use data
from the appropriate rows to graph the necessary plots. 

profile.gperf:
This is the execution profiling report that shows which routine is consuming the most
cycles in my code, and how much time is being spent on each instruction in my routine.
According to this report, it was very clear that my thread function (threadFunc) was
consistently taking over 90% of the cycles, with most of the time being spent on the
spin lock instructions.

lab2b_1.png:
Shows the throughput in operations per second as a function of number of threads for
synchronized adds and list operations. 

lab2b_2.png:
Shows the mean time per mutex wait and per operation for mutex-synchronized list
operations. 

lab2b_3.png:
Shows the number of iterations it takes for my program to reliably fail with 4 lists and
yield options i and d. This is done with and without synchronization.

lab2b_4.png:
Shows the mutex-synchronized throughput of partitioned lists as a function of number of
threads. 

lab2b_5.png:
Shows the spin-lock-synchronized throughput of partitioned lists as a function of number
of threads.

README.txt:
Includes descriptions of each of the included files, and contains all the answers to
the given questions in the spec (included below).

________________________________________________________________________________

Analysis:

QUESTION 2.3.1 - Cycles in the basic implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread tests (for
both add and list)?  Why do you believe these to be the most expensive parts of
the code?
Where do you believe most of the time/cycles are being spent in the high-thread
spin-lock tests?
Where do you believe most of the time/cycles are being spent in the high-thread
mutex tests?

I believe that most of the cycles in both add and list are spent in the thread
function. This is the most expensive part of the code because it has multiple
critical sections in which locks are needed to ensure atomicity. With spin locks
taking up time slices and wasting CPU time due to many threads spinning while
one thread is running the critical section, the thread function takes up many
more cycles than all the other functions combined.

For the high-thread spin-lock tests, most of the cycles are being spent at the
instances of the sync_lock_test_and_set function, which is where the majority of
spinning occurs. This happens at lines 57-59, 97-99, and 138-141 of my code.

For the high-thread mutex tests, most of the cycles are being spent at the
pthread_mutex_lock and pthread_mutex_unlock sections of code. This is because
putting waiting threads to sleep while one thread is running in the critical
section is an expensive operation, in addition to waking threads back up again.
If the period for which a critical section is locked is very short, this expensive
sleep/wakeup problem is especially prevalent, since even spinning might be more
efficient than mutexes in this case.

QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock
version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?

According to the execution profiling report, lines 57 and 138 of my code are
consuming most of the cycles when the 's' syncing option is run with a large
number of 12 threads. Both lines point to this piece of code:

while(__sync_lock_test_and_set(&sync_locks_array[sub_lists[i]], 1));

This is the code that does the spin locking, where waiting threads are constantly
spinning, waiting for it to be unlocked again.

With large numbers of threads, this operation becomes very expensive because as
one thread is running in the critical section, more and more threads are spinning,
waiting for the thread to finish running. This spinning wastes an increasing
amount of CPU time and time slices, and therefore attributes to my thread function
taking up over 90% of all the samples my program takes.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs # threads) and the average wait-for-
mutex time (vs #threads).  
Why does the average lock-wait time rise so dramatically with the number of
contending threads?
Why does the completion time per operation rise (less dramatically) with the
number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than
the completion time per operation?

With fewer threads, there will be a lower average lock-wait time because each thread
will run the critical section a higher percentage of the time. There are fewer threads
that have to wait to run a section, so each thread will get more time to run and less
waiting time. But with more threads, the lock-wait time rises dramatically, since
still only one thread can run the critical section at a time, and an increasing number
of threads are kept waiting, therefore increasing the average wait time.

The average completion time per operation also rises due to the rising expenses of creating
and joining more threads. However, it rises less dramatically compared to the lock-wait
time because for the latter, the increased amount of threads are constantly waiting for more
locks on critical sections, which dramatically increases wait time, whereas the completion
time depends only on how long it takes to create and join threads once they're finished
running.

It's possible for the wait time per operation to exceed the completion time because the former
is measuring CPU time, while the latter is measuring wall time. The waiting time increases more
with a higher amount of threads due to more encounters with locks, but this time is the CPU
time, which can be higher than the wall time calculated by the completion time per operation.
Therefore, even if the lock-wait CPU time gets higher at a faster rate with more threads,
the fact that we're running parallel processes means that the real wall time measured by the
time per operation can remain lower.

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of
the number of lists.
Should the throughput continue increasing as the number of lists is further
increased?  If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should
be equivalent to the throughput of a single list with fewer (1/N) threads.  Does this
appear to be true in the above curves?  If not, explain why not.

As the number of lists increases, the throughput, or operations per second, increases
for the synchronized methods. This is because having more lists means that each sub-list
is smaller, and therefore common operations such as inserts, deletes, and lookups take
less time and fewer cycles. Also with an increasing number of lists, there will be less
conflicts to occur with the same number of threads, because the probability of modifying
the same data structure at the same time becomes low.

The throughput will not continue increasing indefinitely as the number of lists
increases. This can be attributed to an increased overhead associated with managing
more and more smaller lists. For instance, there will be more mutex locking and
unlocking present, which means more waiting time with an increased amount of threads
when finding the list length in the thread function.

It isn't quite true to assume that using an N-way partitioned list with N threads will
have the same throughput as using a single list with a single thread. Although the through-
put values are relatively close in my graphs, it has to be kept in mind that using a single
list with a single thread will be the fastest since we don't have a need to wait for locks,
since there won't be any critical sections. This reduced overhead is why in my graphs,
there is a slightly higher throughput using fewer threads and lists. Also, it has to kept
in mind the graph is logarithmic, which means differences that appear small are actually
non-negligible.
